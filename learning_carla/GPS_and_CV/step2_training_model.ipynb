{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get packages to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-15 14:37:20.050739: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-15 14:37:20.640212: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-15 14:37:20.662952: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/winter/.pyenv/versions/3.7.17/envs/carla-0.9.13-py3.7/lib/python3.7/site-packages/cv2/../../lib64:/home/winter/carla-ros-bridge/install/rviz_carla_plugin/lib:/home/winter/carla-ros-bridge/install/carla_waypoint_types/lib:/home/winter/carla-ros-bridge/install/carla_ros_scenario_runner_types/lib:/home/winter/carla-ros-bridge/install/carla_ackermann_msgs/lib:/home/winter/carla-ros-bridge/install/carla_msgs/lib:/opt/ros/foxy/opt/yaml_cpp_vendor/lib:/opt/ros/foxy/opt/rviz_ogre_vendor/lib:/opt/ros/foxy/lib/x86_64-linux-gnu:/opt/ros/foxy/lib\n",
      "2025-01-15 14:37:20.663063: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-01-15 14:37:22.711543: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/winter/.pyenv/versions/3.7.17/envs/carla-0.9.13-py3.7/lib/python3.7/site-packages/cv2/../../lib64:/home/winter/carla-ros-bridge/install/rviz_carla_plugin/lib:/home/winter/carla-ros-bridge/install/carla_waypoint_types/lib:/home/winter/carla-ros-bridge/install/carla_ros_scenario_runner_types/lib:/home/winter/carla-ros-bridge/install/carla_ackermann_msgs/lib:/home/winter/carla-ros-bridge/install/carla_msgs/lib:/opt/ros/foxy/opt/yaml_cpp_vendor/lib:/opt/ros/foxy/opt/rviz_ogre_vendor/lib:/opt/ros/foxy/lib/x86_64-linux-gnu:/opt/ros/foxy/lib\n",
      "2025-01-15 14:37:22.711869: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/winter/.pyenv/versions/3.7.17/envs/carla-0.9.13-py3.7/lib/python3.7/site-packages/cv2/../../lib64:/home/winter/carla-ros-bridge/install/rviz_carla_plugin/lib:/home/winter/carla-ros-bridge/install/carla_waypoint_types/lib:/home/winter/carla-ros-bridge/install/carla_ros_scenario_runner_types/lib:/home/winter/carla-ros-bridge/install/carla_ackermann_msgs/lib:/home/winter/carla-ros-bridge/install/carla_msgs/lib:/opt/ros/foxy/opt/yaml_cpp_vendor/lib:/opt/ros/foxy/opt/rviz_ogre_vendor/lib:/opt/ros/foxy/lib/x86_64-linux-gnu:/opt/ros/foxy/lib\n",
      "2025-01-15 14:37:22.711887: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import cv2\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, Dropout, MaxPooling2D, Conv2D, Concatenate, Embedding, Reshape, Flatten, Activation\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First attempt to train a model based on images generated\n",
    "- 1130 images\n",
    "- full original image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants to resize image to\n",
    "HEIGHT = 90\n",
    "WIDTH = 160\n",
    "\n",
    "\n",
    "YAW_ADJ_DEGREES = 35 # e.g. goes from -35 to +35\n",
    "\n",
    "\n",
    "#get a lsit of files\n",
    "mypath = '/home/winter/carla-ros-bridge/src/ros-bridge/learning_carla/GPS_and_CV/_img'\n",
    "images = [f.split('.png')[0] for f in os.listdir(mypath) if f.endswith(\".png\")]\n",
    "\n",
    "random.shuffle(images)\n",
    "# get a list when both are available: image and steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training data \n",
    "X = [] #images\n",
    "X1 = [] # gen direction\n",
    "Y = [] #expected steering for this image\n",
    "for example in images:\n",
    "    img_path = mypath+'/'+example+'.png'\n",
    "    image = cv2.imread(img_path,cv2.IMREAD_COLOR)\n",
    "    # option to make images smaller\n",
    "    image = cv2.resize(image, (WIDTH,HEIGHT))\n",
    "    # this version adds taking lower side of the image\n",
    "    X.append(image / 255) # adding another dimension and normalising pixels to 0-1\n",
    "    # gen direction values are taken from after 1st '_' in file name\n",
    "    X1.append(int(example.split('_')[1]))\n",
    "    # y labels are taken from after 2nd '_' in file name\n",
    "    y = float(example.split('_')[2])\n",
    "    # convert to a fraction of 90 degrees so -1 is all the way left and + 1 is all the way right\n",
    "    if y >35:\n",
    "        y = 35\n",
    "    elif y<-35:\n",
    "        y = -35\n",
    "    \n",
    "    y = float(y)/YAW_ADJ_DEGREES # rescale to -1 to +1 so -1 is when max left 35degrees and +1 is +35deg\n",
    "    Y.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to numpy arrays\n",
    "X = np.array(X)\n",
    "X1 = np.array(X1)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 90, 160, 3)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 90, 160, 64)  1792        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 45, 80, 64)   0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 45, 80, 64)   36928       ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 22, 40, 64)  0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 22, 40, 64)   36928       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 11, 20, 64)  0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 14080)        0           ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 14081)        0           ['flatten[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           901248      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            65          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 976,961\n",
      "Trainable params: 976,961\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-15 10:54:26.383110: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:03:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-15 10:54:26.383315: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/winter/.pyenv/versions/3.7.17/envs/carla-0.9.13-py3.7/lib/python3.7/site-packages/cv2/../../lib64:/home/winter/carla-ros-bridge/install/rviz_carla_plugin/lib:/home/winter/carla-ros-bridge/install/carla_waypoint_types/lib:/home/winter/carla-ros-bridge/install/carla_ros_scenario_runner_types/lib:/home/winter/carla-ros-bridge/install/carla_ackermann_msgs/lib:/home/winter/carla-ros-bridge/install/carla_msgs/lib:/opt/ros/foxy/opt/yaml_cpp_vendor/lib:/opt/ros/foxy/opt/rviz_ogre_vendor/lib:/opt/ros/foxy/lib/x86_64-linux-gnu:/opt/ros/foxy/lib\n",
      "2025-01-15 10:54:26.383974: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/winter/.pyenv/versions/3.7.17/envs/carla-0.9.13-py3.7/lib/python3.7/site-packages/cv2/../../lib64:/home/winter/carla-ros-bridge/install/rviz_carla_plugin/lib:/home/winter/carla-ros-bridge/install/carla_waypoint_types/lib:/home/winter/carla-ros-bridge/install/carla_ros_scenario_runner_types/lib:/home/winter/carla-ros-bridge/install/carla_ackermann_msgs/lib:/home/winter/carla-ros-bridge/install/carla_msgs/lib:/opt/ros/foxy/opt/yaml_cpp_vendor/lib:/opt/ros/foxy/opt/rviz_ogre_vendor/lib:/opt/ros/foxy/lib/x86_64-linux-gnu:/opt/ros/foxy/lib\n",
      "2025-01-15 10:54:26.384329: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/winter/.pyenv/versions/3.7.17/envs/carla-0.9.13-py3.7/lib/python3.7/site-packages/cv2/../../lib64:/home/winter/carla-ros-bridge/install/rviz_carla_plugin/lib:/home/winter/carla-ros-bridge/install/carla_waypoint_types/lib:/home/winter/carla-ros-bridge/install/carla_ros_scenario_runner_types/lib:/home/winter/carla-ros-bridge/install/carla_ackermann_msgs/lib:/home/winter/carla-ros-bridge/install/carla_msgs/lib:/opt/ros/foxy/opt/yaml_cpp_vendor/lib:/opt/ros/foxy/opt/rviz_ogre_vendor/lib:/opt/ros/foxy/lib/x86_64-linux-gnu:/opt/ros/foxy/lib\n",
      "2025-01-15 10:54:26.384424: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/winter/.pyenv/versions/3.7.17/envs/carla-0.9.13-py3.7/lib/python3.7/site-packages/cv2/../../lib64:/home/winter/carla-ros-bridge/install/rviz_carla_plugin/lib:/home/winter/carla-ros-bridge/install/carla_waypoint_types/lib:/home/winter/carla-ros-bridge/install/carla_ros_scenario_runner_types/lib:/home/winter/carla-ros-bridge/install/carla_ackermann_msgs/lib:/home/winter/carla-ros-bridge/install/carla_msgs/lib:/opt/ros/foxy/opt/yaml_cpp_vendor/lib:/opt/ros/foxy/opt/rviz_ogre_vendor/lib:/opt/ros/foxy/lib/x86_64-linux-gnu:/opt/ros/foxy/lib\n",
      "2025-01-15 10:54:26.618414: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/winter/.pyenv/versions/3.7.17/envs/carla-0.9.13-py3.7/lib/python3.7/site-packages/cv2/../../lib64:/home/winter/carla-ros-bridge/install/rviz_carla_plugin/lib:/home/winter/carla-ros-bridge/install/carla_waypoint_types/lib:/home/winter/carla-ros-bridge/install/carla_ros_scenario_runner_types/lib:/home/winter/carla-ros-bridge/install/carla_ackermann_msgs/lib:/home/winter/carla-ros-bridge/install/carla_msgs/lib:/opt/ros/foxy/opt/yaml_cpp_vendor/lib:/opt/ros/foxy/opt/rviz_ogre_vendor/lib:/opt/ros/foxy/lib/x86_64-linux-gnu:/opt/ros/foxy/lib\n",
      "2025-01-15 10:54:26.618783: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/winter/.pyenv/versions/3.7.17/envs/carla-0.9.13-py3.7/lib/python3.7/site-packages/cv2/../../lib64:/home/winter/carla-ros-bridge/install/rviz_carla_plugin/lib:/home/winter/carla-ros-bridge/install/carla_waypoint_types/lib:/home/winter/carla-ros-bridge/install/carla_ros_scenario_runner_types/lib:/home/winter/carla-ros-bridge/install/carla_ackermann_msgs/lib:/home/winter/carla-ros-bridge/install/carla_msgs/lib:/opt/ros/foxy/opt/yaml_cpp_vendor/lib:/opt/ros/foxy/opt/rviz_ogre_vendor/lib:/opt/ros/foxy/lib/x86_64-linux-gnu:/opt/ros/foxy/lib\n",
      "2025-01-15 10:54:26.618803: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2025-01-15 10:54:26.619606: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    # Image input\n",
    "    image_input = Input(shape=(HEIGHT, WIDTH, 3))\n",
    "    # Integer input\n",
    "    integer_input = Input(shape=(1,))\n",
    "    \n",
    "    # Preprocess the image input\n",
    "    processed_image = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(image_input)\n",
    "    processed_image = MaxPooling2D(pool_size=(2, 2))(processed_image)\n",
    "\n",
    "    processed_image = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(processed_image)\n",
    "    processed_image = MaxPooling2D(pool_size=(2, 2))(processed_image)\n",
    "\n",
    "    processed_image = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(processed_image)\n",
    "    processed_image = MaxPooling2D(pool_size=(2, 2))(processed_image)\n",
    "\n",
    "    processed_image = Flatten()(processed_image)\n",
    "    \n",
    "    # Concatenate image features with integer input\n",
    "    concatenated_inputs = Concatenate()([processed_image, integer_input])\n",
    "    \n",
    "    # Dense layers for prediction\n",
    "    x = Dense(64, activation='relu')(concatenated_inputs)\n",
    "    output = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=[image_input, integer_input], outputs=output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "57/57 [==============================] - 6s 80ms/step - loss: 0.1831 - MSE: 0.1831 - val_loss: 0.0671 - val_MSE: 0.0671\n",
      "Epoch 2/30\n",
      "57/57 [==============================] - 4s 77ms/step - loss: 0.0660 - MSE: 0.0660 - val_loss: 0.0473 - val_MSE: 0.0473\n",
      "Epoch 3/30\n",
      "57/57 [==============================] - 4s 77ms/step - loss: 0.0328 - MSE: 0.0328 - val_loss: 0.0232 - val_MSE: 0.0232\n",
      "Epoch 4/30\n",
      "57/57 [==============================] - 5s 79ms/step - loss: 0.0198 - MSE: 0.0198 - val_loss: 0.0233 - val_MSE: 0.0233\n",
      "Epoch 5/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0128 - MSE: 0.0128 - val_loss: 0.0182 - val_MSE: 0.0182\n",
      "Epoch 6/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0104 - MSE: 0.0104 - val_loss: 0.0162 - val_MSE: 0.0162\n",
      "Epoch 7/30\n",
      "57/57 [==============================] - 5s 79ms/step - loss: 0.0110 - MSE: 0.0110 - val_loss: 0.0197 - val_MSE: 0.0197\n",
      "Epoch 8/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0116 - MSE: 0.0116 - val_loss: 0.0191 - val_MSE: 0.0191\n",
      "Epoch 9/30\n",
      "57/57 [==============================] - 5s 81ms/step - loss: 0.0083 - MSE: 0.0083 - val_loss: 0.0128 - val_MSE: 0.0128\n",
      "Epoch 10/30\n",
      "57/57 [==============================] - 5s 81ms/step - loss: 0.0084 - MSE: 0.0084 - val_loss: 0.0107 - val_MSE: 0.0107\n",
      "Epoch 11/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0081 - MSE: 0.0081 - val_loss: 0.0118 - val_MSE: 0.0118\n",
      "Epoch 12/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0065 - MSE: 0.0065 - val_loss: 0.0110 - val_MSE: 0.0110\n",
      "Epoch 13/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0053 - MSE: 0.0053 - val_loss: 0.0097 - val_MSE: 0.0097\n",
      "Epoch 14/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0052 - MSE: 0.0052 - val_loss: 0.0113 - val_MSE: 0.0113\n",
      "Epoch 15/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0065 - MSE: 0.0065 - val_loss: 0.0175 - val_MSE: 0.0175\n",
      "Epoch 16/30\n",
      "57/57 [==============================] - 5s 81ms/step - loss: 0.0067 - MSE: 0.0067 - val_loss: 0.0133 - val_MSE: 0.0133\n",
      "Epoch 17/30\n",
      "57/57 [==============================] - 5s 81ms/step - loss: 0.0066 - MSE: 0.0066 - val_loss: 0.0107 - val_MSE: 0.0107\n",
      "Epoch 18/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0049 - MSE: 0.0049 - val_loss: 0.0141 - val_MSE: 0.0141\n",
      "Epoch 19/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0057 - MSE: 0.0057 - val_loss: 0.0111 - val_MSE: 0.0111\n",
      "Epoch 20/30\n",
      "57/57 [==============================] - 5s 81ms/step - loss: 0.0046 - MSE: 0.0046 - val_loss: 0.0099 - val_MSE: 0.0099\n",
      "Epoch 21/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0039 - MSE: 0.0039 - val_loss: 0.0087 - val_MSE: 0.0087\n",
      "Epoch 22/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0024 - MSE: 0.0024 - val_loss: 0.0087 - val_MSE: 0.0087\n",
      "Epoch 23/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0019 - MSE: 0.0019 - val_loss: 0.0111 - val_MSE: 0.0111\n",
      "Epoch 24/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0020 - MSE: 0.0020 - val_loss: 0.0105 - val_MSE: 0.0105\n",
      "Epoch 25/30\n",
      "57/57 [==============================] - 5s 81ms/step - loss: 0.0024 - MSE: 0.0024 - val_loss: 0.0093 - val_MSE: 0.0093\n",
      "Epoch 26/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0026 - MSE: 0.0026 - val_loss: 0.0087 - val_MSE: 0.0087\n",
      "Epoch 27/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0028 - MSE: 0.0028 - val_loss: 0.0090 - val_MSE: 0.0090\n",
      "Epoch 28/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0022 - MSE: 0.0022 - val_loss: 0.0087 - val_MSE: 0.0087\n",
      "Epoch 29/30\n",
      "57/57 [==============================] - 5s 81ms/step - loss: 0.0020 - MSE: 0.0020 - val_loss: 0.0141 - val_MSE: 0.0141\n",
      "Epoch 30/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0027 - MSE: 0.0027 - val_loss: 0.0132 - val_MSE: 0.0132\n",
      "36/36 [==============================] - 2s 46ms/step\n",
      "Prediction min:  -1.0260559  Prediction max:  1.1097745\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['MSE'])\n",
    "\n",
    "\n",
    "model.fit([X, X1], Y, batch_size=16, shuffle=False, epochs=30, validation_split=0.2)\n",
    "\n",
    "predictions = model.predict([X,X1])\n",
    "print(\"Prediction min: \",predictions.min(),\" Prediction max: \",predictions.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's check distribution of our labels \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "frq, edges = np.histogram(Y, bins=20)\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(edges[:-1], frq, width=np.diff(edges), edgecolor=\"black\", align=\"edge\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quite well distributed!\n",
    "\n",
    "Let's save the model and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: GPS_Visual_Model_overfit/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: GPS_Visual_Model_overfit/assets\n"
     ]
    }
   ],
   "source": [
    "# to save model\n",
    "model.save(\"GPS_Visual_Model_overfit\", overwrite=True,include_optimizer=True,\n",
    "    save_format=None, signatures=None, options=None, save_traces=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model shows poor performance. It seems like drunk.\n",
    "\n",
    "\n",
    "I need to collect more images(current : 1130) and balance those training set across steering angles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second attempt to train a model based on images generated\n",
    "- 2420 images\n",
    "- balanced image data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants to resize image to\n",
    "HEIGHT = 180\n",
    "WIDTH = 320\n",
    "\n",
    "YAW_ADJ_DEGREES = 35 # e.g. goes from -35 to +35\n",
    "\n",
    "#get a lsit of files\n",
    "mypath = '/home/winter/carla-ros-bridge/src/ros-bridge/learning_carla/GPS_and_CV/_img'\n",
    "images = [f.split('.png')[0] for f in os.listdir(mypath) if f.endswith(\".png\")]\n",
    "\n",
    "random.shuffle(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_array(bin_start,bin_end,bin_size):\n",
    "    '''\n",
    "    This function returns indicies of selected elements \n",
    "    which make the training set balanced\n",
    "    You need to apply the returned index to all training arrays  \n",
    "    '''\n",
    "    num_bins = int((bin_end - bin_start) / bin_size) + 1\n",
    "    min_count = np.min(np.histogram(Y, bins=num_bins, range=(bin_start, bin_end))[0])\n",
    "    balanced_array = []\n",
    "    selected = []\n",
    "\n",
    "    for start in np.arange(bin_start, bin_end, bin_size):\n",
    "        end = start + bin_size\n",
    "        indices = np.where((Y >= start) & (Y < end))[0]\n",
    "        #balanced_array.extend(Y[indices[:min_count]])\n",
    "        selected.extend(indices[:min_count])\n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [] #images\n",
    "X1 = [] # gen direction\n",
    "Y = [] # expected steering for this image\n",
    "for example in images:\n",
    "    img_path = mypath+'/'+example+'.png'\n",
    "    image = cv2.imread(img_path,cv2.IMREAD_COLOR)\n",
    "    # option to make images smaller\n",
    "    image = cv2.resize(image, (WIDTH,HEIGHT))\n",
    "    \n",
    "    # y labels are taken from after 2nd '_' in file name\n",
    "    y = float(example.split('_')[2])\n",
    "    # convert to a fraction of 90 degrees so -1 is all the way left and + 1 is all the way right\n",
    "    if y >35:\n",
    "        y = 35\n",
    "    elif y<-35:\n",
    "        y = -35\n",
    "    \n",
    "    y = float(y)/YAW_ADJ_DEGREES # rescale to -1 to +1 so -1 is when max left 35degrees and +1 is +35deg\n",
    "    # a rough balancing by reducing number of zero steer examples\n",
    "    X.append(image / 255) # adding another dimension and normalising pixels to 0-1\n",
    "    # gen direction values are taken from after 1st '_' in file name\n",
    "    X1.append(int(example.split('_')[1]))\n",
    "    Y.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to numpy arrays\n",
    "X = np.array(X)\n",
    "X1 = np.array(X1)\n",
    "Y = np.array(Y)\n",
    "\n",
    "balanced_subset = balance_array(-0.5,0.5,0.05)\n",
    "\n",
    "X = X[balanced_subset]\n",
    "X1 = X1[balanced_subset]\n",
    "Y = Y[balanced_subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is how training set is distributed:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# draw how Y is distributed befored going into training\n",
    "frq, edges = np.histogram(Y, bins=20)\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(edges[:-1], frq, width=np.diff(edges), edgecolor=\"black\", align=\"edge\")\n",
    "print('This is how training set is distributed:')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-15 14:43:01.037646: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:03:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-15 14:43:01.038176: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/winter/.pyenv/versions/3.7.17/envs/carla-0.9.13-py3.7/lib/python3.7/site-packages/cv2/../../lib64:/home/winter/carla-ros-bridge/install/rviz_carla_plugin/lib:/home/winter/carla-ros-bridge/install/carla_waypoint_types/lib:/home/winter/carla-ros-bridge/install/carla_ros_scenario_runner_types/lib:/home/winter/carla-ros-bridge/install/carla_ackermann_msgs/lib:/home/winter/carla-ros-bridge/install/carla_msgs/lib:/opt/ros/foxy/opt/yaml_cpp_vendor/lib:/opt/ros/foxy/opt/rviz_ogre_vendor/lib:/opt/ros/foxy/lib/x86_64-linux-gnu:/opt/ros/foxy/lib\n",
      "2025-01-15 14:43:01.038289: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/winter/.pyenv/versions/3.7.17/envs/carla-0.9.13-py3.7/lib/python3.7/site-packages/cv2/../../lib64:/home/winter/carla-ros-bridge/install/rviz_carla_plugin/lib:/home/winter/carla-ros-bridge/install/carla_waypoint_types/lib:/home/winter/carla-ros-bridge/install/carla_ros_scenario_runner_types/lib:/home/winter/carla-ros-bridge/install/carla_ackermann_msgs/lib:/home/winter/carla-ros-bridge/install/carla_msgs/lib:/opt/ros/foxy/opt/yaml_cpp_vendor/lib:/opt/ros/foxy/opt/rviz_ogre_vendor/lib:/opt/ros/foxy/lib/x86_64-linux-gnu:/opt/ros/foxy/lib\n",
      "2025-01-15 14:43:01.038360: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/winter/.pyenv/versions/3.7.17/envs/carla-0.9.13-py3.7/lib/python3.7/site-packages/cv2/../../lib64:/home/winter/carla-ros-bridge/install/rviz_carla_plugin/lib:/home/winter/carla-ros-bridge/install/carla_waypoint_types/lib:/home/winter/carla-ros-bridge/install/carla_ros_scenario_runner_types/lib:/home/winter/carla-ros-bridge/install/carla_ackermann_msgs/lib:/home/winter/carla-ros-bridge/install/carla_msgs/lib:/opt/ros/foxy/opt/yaml_cpp_vendor/lib:/opt/ros/foxy/opt/rviz_ogre_vendor/lib:/opt/ros/foxy/lib/x86_64-linux-gnu:/opt/ros/foxy/lib\n",
      "2025-01-15 14:43:01.038422: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/winter/.pyenv/versions/3.7.17/envs/carla-0.9.13-py3.7/lib/python3.7/site-packages/cv2/../../lib64:/home/winter/carla-ros-bridge/install/rviz_carla_plugin/lib:/home/winter/carla-ros-bridge/install/carla_waypoint_types/lib:/home/winter/carla-ros-bridge/install/carla_ros_scenario_runner_types/lib:/home/winter/carla-ros-bridge/install/carla_ackermann_msgs/lib:/home/winter/carla-ros-bridge/install/carla_msgs/lib:/opt/ros/foxy/opt/yaml_cpp_vendor/lib:/opt/ros/foxy/opt/rviz_ogre_vendor/lib:/opt/ros/foxy/lib/x86_64-linux-gnu:/opt/ros/foxy/lib\n",
      "2025-01-15 14:43:01.351275: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/winter/.pyenv/versions/3.7.17/envs/carla-0.9.13-py3.7/lib/python3.7/site-packages/cv2/../../lib64:/home/winter/carla-ros-bridge/install/rviz_carla_plugin/lib:/home/winter/carla-ros-bridge/install/carla_waypoint_types/lib:/home/winter/carla-ros-bridge/install/carla_ros_scenario_runner_types/lib:/home/winter/carla-ros-bridge/install/carla_ackermann_msgs/lib:/home/winter/carla-ros-bridge/install/carla_msgs/lib:/opt/ros/foxy/opt/yaml_cpp_vendor/lib:/opt/ros/foxy/opt/rviz_ogre_vendor/lib:/opt/ros/foxy/lib/x86_64-linux-gnu:/opt/ros/foxy/lib\n",
      "2025-01-15 14:43:01.351418: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/winter/.pyenv/versions/3.7.17/envs/carla-0.9.13-py3.7/lib/python3.7/site-packages/cv2/../../lib64:/home/winter/carla-ros-bridge/install/rviz_carla_plugin/lib:/home/winter/carla-ros-bridge/install/carla_waypoint_types/lib:/home/winter/carla-ros-bridge/install/carla_ros_scenario_runner_types/lib:/home/winter/carla-ros-bridge/install/carla_ackermann_msgs/lib:/home/winter/carla-ros-bridge/install/carla_msgs/lib:/opt/ros/foxy/opt/yaml_cpp_vendor/lib:/opt/ros/foxy/opt/rviz_ogre_vendor/lib:/opt/ros/foxy/lib/x86_64-linux-gnu:/opt/ros/foxy/lib\n",
      "2025-01-15 14:43:01.351560: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2025-01-15 14:43:01.353791: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 180, 320, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 180, 320, 64  1792        ['input_1[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 90, 160, 64)  0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 90, 160, 64)  36928       ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 45, 80, 64)  0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 45, 80, 64)   36928       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 22, 40, 64)  0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 56320)        0           ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 56321)        0           ['flatten[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           3604608     ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            65          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 3,680,321\n",
      "Trainable params: 3,680,321\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    # Image input\n",
    "    image_input = Input(shape=(HEIGHT, WIDTH, 3))\n",
    "    # Integer input\n",
    "    integer_input = Input(shape=(1,))\n",
    "    \n",
    "    # Preprocess the image input\n",
    "    processed_image = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(image_input)\n",
    "    processed_image = MaxPooling2D(pool_size=(2, 2))(processed_image)\n",
    "\n",
    "    processed_image = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(processed_image)\n",
    "    processed_image = MaxPooling2D(pool_size=(2, 2))(processed_image)\n",
    "\n",
    "    processed_image = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(processed_image)\n",
    "    processed_image = MaxPooling2D(pool_size=(2, 2))(processed_image)\n",
    "\n",
    "    processed_image = Flatten()(processed_image)\n",
    "    \n",
    "    # Concatenate image features with integer input\n",
    "    concatenated_inputs = Concatenate()([processed_image, integer_input])\n",
    "    \n",
    "    # Dense layers for prediction\n",
    "    x = Dense(64, activation='relu')(concatenated_inputs)\n",
    "    output = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=[image_input, integer_input], outputs=output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "43/43 [==============================] - 21s 415ms/step - loss: 2.0003 - MSE: 2.0003 - val_loss: 0.0087 - val_MSE: 0.0087\n",
      "Epoch 2/10\n",
      "43/43 [==============================] - 16s 379ms/step - loss: 0.0773 - MSE: 0.0773 - val_loss: 0.1486 - val_MSE: 0.1486\n",
      "Epoch 3/10\n",
      "43/43 [==============================] - 17s 386ms/step - loss: 0.0674 - MSE: 0.0674 - val_loss: 0.1713 - val_MSE: 0.1713\n",
      "Epoch 4/10\n",
      "43/43 [==============================] - 17s 390ms/step - loss: 0.0621 - MSE: 0.0621 - val_loss: 0.1817 - val_MSE: 0.1817\n",
      "Epoch 5/10\n",
      "43/43 [==============================] - 17s 389ms/step - loss: 0.0563 - MSE: 0.0563 - val_loss: 0.1890 - val_MSE: 0.1890\n",
      "Epoch 6/10\n",
      "43/43 [==============================] - 17s 388ms/step - loss: 0.0097 - MSE: 0.0097 - val_loss: 0.0172 - val_MSE: 0.0172\n",
      "Epoch 7/10\n",
      "43/43 [==============================] - 16s 374ms/step - loss: 0.0707 - MSE: 0.0707 - val_loss: 0.0380 - val_MSE: 0.0380\n",
      "Epoch 8/10\n",
      "43/43 [==============================] - 16s 373ms/step - loss: 0.0811 - MSE: 0.0811 - val_loss: 0.1886 - val_MSE: 0.1886\n",
      "Epoch 9/10\n",
      "43/43 [==============================] - 16s 364ms/step - loss: 0.0349 - MSE: 0.0349 - val_loss: 0.1408 - val_MSE: 0.1408\n",
      "Epoch 10/10\n",
      "43/43 [==============================] - 14s 328ms/step - loss: 0.0432 - MSE: 0.0432 - val_loss: 0.0065 - val_MSE: 0.0065\n",
      "27/27 [==============================] - 5s 180ms/step\n",
      "Prediction min:  0.30781674  Prediction max:  0.3841688\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='MSE',\n",
    "              optimizer='adam',\n",
    "              metrics=['MSE'])\n",
    "\n",
    "\n",
    "model.fit([X, X1], Y, batch_size=16, shuffle=False, epochs=10, validation_split=0.2)\n",
    "\n",
    "predictions = model.predict([X,X1])\n",
    "print(\"Prediction min: \",predictions.min(),\" Prediction max: \",predictions.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: GPS_Visual_Model_balanced/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: GPS_Visual_Model_balanced/assets\n"
     ]
    }
   ],
   "source": [
    "# to save model\n",
    "model.save(\"GPS_Visual_Model_balanced\", overwrite=True,include_optimizer=True,\n",
    "    save_format=None, signatures=None, options=None, save_traces=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carla-0.9.13-py3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
