{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get packages to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-15 10:48:31.218371: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-15 10:48:31.313407: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-15 10:48:31.319228: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/winter/.pyenv/versions/3.7.17/envs/carla-0.9.13-py3.7/lib/python3.7/site-packages/cv2/../../lib64:/home/winter/carla-ros-bridge/install/rviz_carla_plugin/lib:/home/winter/carla-ros-bridge/install/carla_waypoint_types/lib:/home/winter/carla-ros-bridge/install/carla_ros_scenario_runner_types/lib:/home/winter/carla-ros-bridge/install/carla_ackermann_msgs/lib:/home/winter/carla-ros-bridge/install/carla_msgs/lib:/opt/ros/foxy/opt/yaml_cpp_vendor/lib:/opt/ros/foxy/opt/rviz_ogre_vendor/lib:/opt/ros/foxy/lib/x86_64-linux-gnu:/opt/ros/foxy/lib\n",
      "2025-01-15 10:48:31.319247: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2025-01-15 10:48:31.867966: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/winter/.pyenv/versions/3.7.17/envs/carla-0.9.13-py3.7/lib/python3.7/site-packages/cv2/../../lib64:/home/winter/carla-ros-bridge/install/rviz_carla_plugin/lib:/home/winter/carla-ros-bridge/install/carla_waypoint_types/lib:/home/winter/carla-ros-bridge/install/carla_ros_scenario_runner_types/lib:/home/winter/carla-ros-bridge/install/carla_ackermann_msgs/lib:/home/winter/carla-ros-bridge/install/carla_msgs/lib:/opt/ros/foxy/opt/yaml_cpp_vendor/lib:/opt/ros/foxy/opt/rviz_ogre_vendor/lib:/opt/ros/foxy/lib/x86_64-linux-gnu:/opt/ros/foxy/lib\n",
      "2025-01-15 10:48:31.868762: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/winter/.pyenv/versions/3.7.17/envs/carla-0.9.13-py3.7/lib/python3.7/site-packages/cv2/../../lib64:/home/winter/carla-ros-bridge/install/rviz_carla_plugin/lib:/home/winter/carla-ros-bridge/install/carla_waypoint_types/lib:/home/winter/carla-ros-bridge/install/carla_ros_scenario_runner_types/lib:/home/winter/carla-ros-bridge/install/carla_ackermann_msgs/lib:/home/winter/carla-ros-bridge/install/carla_msgs/lib:/opt/ros/foxy/opt/yaml_cpp_vendor/lib:/opt/ros/foxy/opt/rviz_ogre_vendor/lib:/opt/ros/foxy/lib/x86_64-linux-gnu:/opt/ros/foxy/lib\n",
      "2025-01-15 10:48:31.868783: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import cv2\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Input, Dropout, MaxPooling2D, Conv2D, Concatenate, Embedding, Reshape, Flatten, Activation\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First attempt to train a model based on images generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constants to resize image to\n",
    "HEIGHT = 90\n",
    "WIDTH = 160\n",
    "\n",
    "\n",
    "YAW_ADJ_DEGREES = 35 # e.g. goes from -35 to +35\n",
    "\n",
    "\n",
    "#get a lsit of files\n",
    "mypath = '/home/winter/carla-ros-bridge/src/ros-bridge/learning_carla/GPS_and_CV/_img'\n",
    "images = [f.split('.png')[0] for f in os.listdir(mypath) if f.endswith(\".png\")]\n",
    "\n",
    "random.shuffle(images)\n",
    "# get a list when both are available: image and steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training data \n",
    "X = [] #images\n",
    "X1 = [] # gen direction\n",
    "Y = [] #expected steering for this image\n",
    "for example in images:\n",
    "    img_path = mypath+'/'+example+'.png'\n",
    "    image = cv2.imread(img_path,cv2.IMREAD_COLOR)\n",
    "    # option to make images smaller\n",
    "    image = cv2.resize(image, (WIDTH,HEIGHT))\n",
    "    # this version adds taking lower side of the image\n",
    "    X.append(image / 255) # adding another dimension and normalising pixels to 0-1\n",
    "    # gen direction values are taken from after 1st '_' in file name\n",
    "    X1.append(int(example.split('_')[1]))\n",
    "    # y labels are taken from after 2nd '_' in file name\n",
    "    y = float(example.split('_')[2])\n",
    "    # convert to a fraction of 90 degrees so -1 is all the way left and + 1 is all the way right\n",
    "    if y >35:\n",
    "        y = 35\n",
    "    elif y<-35:\n",
    "        y = -35\n",
    "    \n",
    "    y = float(y)/YAW_ADJ_DEGREES # rescale to -1 to +1 so -1 is when max left 35degrees and +1 is +35deg\n",
    "    Y.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert to numpy arrays\n",
    "X = np.array(X)\n",
    "X1 = np.array(X1)\n",
    "Y = np.array(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 90, 160, 3)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 90, 160, 64)  1792        ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 45, 80, 64)   0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 45, 80, 64)   36928       ['max_pooling2d[0][0]']          \n",
      "                                                                                                  \n",
      " max_pooling2d_1 (MaxPooling2D)  (None, 22, 40, 64)  0           ['conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 22, 40, 64)   36928       ['max_pooling2d_1[0][0]']        \n",
      "                                                                                                  \n",
      " max_pooling2d_2 (MaxPooling2D)  (None, 11, 20, 64)  0           ['conv2d_2[0][0]']               \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 14080)        0           ['max_pooling2d_2[0][0]']        \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 14081)        0           ['flatten[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           901248      ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            65          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 976,961\n",
      "Trainable params: 976,961\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-15 10:54:26.383110: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:967] could not open file to read NUMA node: /sys/bus/pci/devices/0000:03:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-01-15 10:54:26.383315: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/winter/.pyenv/versions/3.7.17/envs/carla-0.9.13-py3.7/lib/python3.7/site-packages/cv2/../../lib64:/home/winter/carla-ros-bridge/install/rviz_carla_plugin/lib:/home/winter/carla-ros-bridge/install/carla_waypoint_types/lib:/home/winter/carla-ros-bridge/install/carla_ros_scenario_runner_types/lib:/home/winter/carla-ros-bridge/install/carla_ackermann_msgs/lib:/home/winter/carla-ros-bridge/install/carla_msgs/lib:/opt/ros/foxy/opt/yaml_cpp_vendor/lib:/opt/ros/foxy/opt/rviz_ogre_vendor/lib:/opt/ros/foxy/lib/x86_64-linux-gnu:/opt/ros/foxy/lib\n",
      "2025-01-15 10:54:26.383974: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/winter/.pyenv/versions/3.7.17/envs/carla-0.9.13-py3.7/lib/python3.7/site-packages/cv2/../../lib64:/home/winter/carla-ros-bridge/install/rviz_carla_plugin/lib:/home/winter/carla-ros-bridge/install/carla_waypoint_types/lib:/home/winter/carla-ros-bridge/install/carla_ros_scenario_runner_types/lib:/home/winter/carla-ros-bridge/install/carla_ackermann_msgs/lib:/home/winter/carla-ros-bridge/install/carla_msgs/lib:/opt/ros/foxy/opt/yaml_cpp_vendor/lib:/opt/ros/foxy/opt/rviz_ogre_vendor/lib:/opt/ros/foxy/lib/x86_64-linux-gnu:/opt/ros/foxy/lib\n",
      "2025-01-15 10:54:26.384329: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/winter/.pyenv/versions/3.7.17/envs/carla-0.9.13-py3.7/lib/python3.7/site-packages/cv2/../../lib64:/home/winter/carla-ros-bridge/install/rviz_carla_plugin/lib:/home/winter/carla-ros-bridge/install/carla_waypoint_types/lib:/home/winter/carla-ros-bridge/install/carla_ros_scenario_runner_types/lib:/home/winter/carla-ros-bridge/install/carla_ackermann_msgs/lib:/home/winter/carla-ros-bridge/install/carla_msgs/lib:/opt/ros/foxy/opt/yaml_cpp_vendor/lib:/opt/ros/foxy/opt/rviz_ogre_vendor/lib:/opt/ros/foxy/lib/x86_64-linux-gnu:/opt/ros/foxy/lib\n",
      "2025-01-15 10:54:26.384424: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/winter/.pyenv/versions/3.7.17/envs/carla-0.9.13-py3.7/lib/python3.7/site-packages/cv2/../../lib64:/home/winter/carla-ros-bridge/install/rviz_carla_plugin/lib:/home/winter/carla-ros-bridge/install/carla_waypoint_types/lib:/home/winter/carla-ros-bridge/install/carla_ros_scenario_runner_types/lib:/home/winter/carla-ros-bridge/install/carla_ackermann_msgs/lib:/home/winter/carla-ros-bridge/install/carla_msgs/lib:/opt/ros/foxy/opt/yaml_cpp_vendor/lib:/opt/ros/foxy/opt/rviz_ogre_vendor/lib:/opt/ros/foxy/lib/x86_64-linux-gnu:/opt/ros/foxy/lib\n",
      "2025-01-15 10:54:26.618414: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/winter/.pyenv/versions/3.7.17/envs/carla-0.9.13-py3.7/lib/python3.7/site-packages/cv2/../../lib64:/home/winter/carla-ros-bridge/install/rviz_carla_plugin/lib:/home/winter/carla-ros-bridge/install/carla_waypoint_types/lib:/home/winter/carla-ros-bridge/install/carla_ros_scenario_runner_types/lib:/home/winter/carla-ros-bridge/install/carla_ackermann_msgs/lib:/home/winter/carla-ros-bridge/install/carla_msgs/lib:/opt/ros/foxy/opt/yaml_cpp_vendor/lib:/opt/ros/foxy/opt/rviz_ogre_vendor/lib:/opt/ros/foxy/lib/x86_64-linux-gnu:/opt/ros/foxy/lib\n",
      "2025-01-15 10:54:26.618783: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/winter/.pyenv/versions/3.7.17/envs/carla-0.9.13-py3.7/lib/python3.7/site-packages/cv2/../../lib64:/home/winter/carla-ros-bridge/install/rviz_carla_plugin/lib:/home/winter/carla-ros-bridge/install/carla_waypoint_types/lib:/home/winter/carla-ros-bridge/install/carla_ros_scenario_runner_types/lib:/home/winter/carla-ros-bridge/install/carla_ackermann_msgs/lib:/home/winter/carla-ros-bridge/install/carla_msgs/lib:/opt/ros/foxy/opt/yaml_cpp_vendor/lib:/opt/ros/foxy/opt/rviz_ogre_vendor/lib:/opt/ros/foxy/lib/x86_64-linux-gnu:/opt/ros/foxy/lib\n",
      "2025-01-15 10:54:26.618803: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1934] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2025-01-15 10:54:26.619606: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    # Image input\n",
    "    image_input = Input(shape=(HEIGHT, WIDTH, 3))\n",
    "    # Integer input\n",
    "    integer_input = Input(shape=(1,))\n",
    "    \n",
    "    # Preprocess the image input\n",
    "    processed_image = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(image_input)\n",
    "    processed_image = MaxPooling2D(pool_size=(2, 2))(processed_image)\n",
    "\n",
    "    processed_image = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(processed_image)\n",
    "    processed_image = MaxPooling2D(pool_size=(2, 2))(processed_image)\n",
    "\n",
    "    processed_image = Conv2D(64, kernel_size=(3, 3), activation='relu',padding='same')(processed_image)\n",
    "    processed_image = MaxPooling2D(pool_size=(2, 2))(processed_image)\n",
    "\n",
    "    processed_image = Flatten()(processed_image)\n",
    "    \n",
    "    # Concatenate image features with integer input\n",
    "    concatenated_inputs = Concatenate()([processed_image, integer_input])\n",
    "    \n",
    "    # Dense layers for prediction\n",
    "    x = Dense(64, activation='relu')(concatenated_inputs)\n",
    "    output = Dense(1, activation='linear')(x)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=[image_input, integer_input], outputs=output)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = create_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "57/57 [==============================] - 6s 80ms/step - loss: 0.1831 - MSE: 0.1831 - val_loss: 0.0671 - val_MSE: 0.0671\n",
      "Epoch 2/30\n",
      "57/57 [==============================] - 4s 77ms/step - loss: 0.0660 - MSE: 0.0660 - val_loss: 0.0473 - val_MSE: 0.0473\n",
      "Epoch 3/30\n",
      "57/57 [==============================] - 4s 77ms/step - loss: 0.0328 - MSE: 0.0328 - val_loss: 0.0232 - val_MSE: 0.0232\n",
      "Epoch 4/30\n",
      "57/57 [==============================] - 5s 79ms/step - loss: 0.0198 - MSE: 0.0198 - val_loss: 0.0233 - val_MSE: 0.0233\n",
      "Epoch 5/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0128 - MSE: 0.0128 - val_loss: 0.0182 - val_MSE: 0.0182\n",
      "Epoch 6/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0104 - MSE: 0.0104 - val_loss: 0.0162 - val_MSE: 0.0162\n",
      "Epoch 7/30\n",
      "57/57 [==============================] - 5s 79ms/step - loss: 0.0110 - MSE: 0.0110 - val_loss: 0.0197 - val_MSE: 0.0197\n",
      "Epoch 8/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0116 - MSE: 0.0116 - val_loss: 0.0191 - val_MSE: 0.0191\n",
      "Epoch 9/30\n",
      "57/57 [==============================] - 5s 81ms/step - loss: 0.0083 - MSE: 0.0083 - val_loss: 0.0128 - val_MSE: 0.0128\n",
      "Epoch 10/30\n",
      "57/57 [==============================] - 5s 81ms/step - loss: 0.0084 - MSE: 0.0084 - val_loss: 0.0107 - val_MSE: 0.0107\n",
      "Epoch 11/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0081 - MSE: 0.0081 - val_loss: 0.0118 - val_MSE: 0.0118\n",
      "Epoch 12/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0065 - MSE: 0.0065 - val_loss: 0.0110 - val_MSE: 0.0110\n",
      "Epoch 13/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0053 - MSE: 0.0053 - val_loss: 0.0097 - val_MSE: 0.0097\n",
      "Epoch 14/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0052 - MSE: 0.0052 - val_loss: 0.0113 - val_MSE: 0.0113\n",
      "Epoch 15/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0065 - MSE: 0.0065 - val_loss: 0.0175 - val_MSE: 0.0175\n",
      "Epoch 16/30\n",
      "57/57 [==============================] - 5s 81ms/step - loss: 0.0067 - MSE: 0.0067 - val_loss: 0.0133 - val_MSE: 0.0133\n",
      "Epoch 17/30\n",
      "57/57 [==============================] - 5s 81ms/step - loss: 0.0066 - MSE: 0.0066 - val_loss: 0.0107 - val_MSE: 0.0107\n",
      "Epoch 18/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0049 - MSE: 0.0049 - val_loss: 0.0141 - val_MSE: 0.0141\n",
      "Epoch 19/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0057 - MSE: 0.0057 - val_loss: 0.0111 - val_MSE: 0.0111\n",
      "Epoch 20/30\n",
      "57/57 [==============================] - 5s 81ms/step - loss: 0.0046 - MSE: 0.0046 - val_loss: 0.0099 - val_MSE: 0.0099\n",
      "Epoch 21/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0039 - MSE: 0.0039 - val_loss: 0.0087 - val_MSE: 0.0087\n",
      "Epoch 22/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0024 - MSE: 0.0024 - val_loss: 0.0087 - val_MSE: 0.0087\n",
      "Epoch 23/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0019 - MSE: 0.0019 - val_loss: 0.0111 - val_MSE: 0.0111\n",
      "Epoch 24/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0020 - MSE: 0.0020 - val_loss: 0.0105 - val_MSE: 0.0105\n",
      "Epoch 25/30\n",
      "57/57 [==============================] - 5s 81ms/step - loss: 0.0024 - MSE: 0.0024 - val_loss: 0.0093 - val_MSE: 0.0093\n",
      "Epoch 26/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0026 - MSE: 0.0026 - val_loss: 0.0087 - val_MSE: 0.0087\n",
      "Epoch 27/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0028 - MSE: 0.0028 - val_loss: 0.0090 - val_MSE: 0.0090\n",
      "Epoch 28/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0022 - MSE: 0.0022 - val_loss: 0.0087 - val_MSE: 0.0087\n",
      "Epoch 29/30\n",
      "57/57 [==============================] - 5s 81ms/step - loss: 0.0020 - MSE: 0.0020 - val_loss: 0.0141 - val_MSE: 0.0141\n",
      "Epoch 30/30\n",
      "57/57 [==============================] - 5s 80ms/step - loss: 0.0027 - MSE: 0.0027 - val_loss: 0.0132 - val_MSE: 0.0132\n",
      "36/36 [==============================] - 2s 46ms/step\n",
      "Prediction min:  -1.0260559  Prediction max:  1.1097745\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='mean_squared_error',\n",
    "              optimizer='adam',\n",
    "              metrics=['MSE'])\n",
    "\n",
    "\n",
    "model.fit([X, X1], Y, batch_size=16, shuffle=False, epochs=30, validation_split=0.2)\n",
    "\n",
    "predictions = model.predict([X,X1])\n",
    "print(\"Prediction min: \",predictions.min(),\" Prediction max: \",predictions.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's check distribution of our labels \n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "frq, edges = np.histogram(Y, bins=20)\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(edges[:-1], frq, width=np.diff(edges), edgecolor=\"black\", align=\"edge\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quite well distributed!\n",
    "\n",
    "Let's save the model and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: GPS_Visual_Model_overfit/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: GPS_Visual_Model_overfit/assets\n"
     ]
    }
   ],
   "source": [
    "# to save model\n",
    "model.save(\"GPS_Visual_Model_overfit\", overwrite=True,include_optimizer=True,\n",
    "    save_format=None, signatures=None, options=None, save_traces=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "carla-0.9.13-py3.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
